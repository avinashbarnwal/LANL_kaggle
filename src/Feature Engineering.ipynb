{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom joblib import Parallel, delayed\nimport os\nimport gc\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nimport scipy as sp\nfrom sklearn import metrics\nfrom tsfresh.feature_extraction import feature_calculators\newma = pd.Series.ewm\nimport os\nprint(os.listdir(\"../input\"))\nno_of_std = 3\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain      = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\nsubmission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a training file with simple derived features\nrows     = 150_000\nsegments = int(np.floor(train.shape[0] / rows))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg = pd.read_csv('../input/test/' + 'seg_00030f' + '.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg.rolling(window=700).mean().mean(skipna=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureGenerator(object):\n    def __init__(self, dtype, n_jobs=1, chunk_size=None):\n        self.chunk_size = chunk_size\n        self.dtype = dtype\n        self.filename = None\n        self.n_jobs = n_jobs\n        self.test_files = []\n        \n        if self.dtype == 'train':\n            self.filename = '../input/train.csv'\n            self.total_data = int(629145481 / self.chunk_size)\n        else:\n            submission = pd.read_csv('../input/sample_submission.csv')\n            for seg_id in submission.seg_id.values:\n                self.test_files.append((seg_id, '../input/test/' + seg_id + '.csv'))\n            self.total_data = int(len(submission))\n\n    def read_chunks(self):\n        if self.dtype == 'train':\n            iter_df = pd.read_csv(self.filename, iterator=True, chunksize=self.chunk_size,\n                                  dtype={'acoustic_data': np.float64, 'time_to_failure': np.float64})\n            for counter, df in enumerate(iter_df):\n                x = df.acoustic_data.values\n                y = df.time_to_failure.values[-1]\n                seg_id = 'train_' + str(counter)\n                del df\n                yield seg_id, x, y\n        else:\n            for seg_id, f in self.test_files:\n                df = pd.read_csv(f, dtype={'acoustic_data': np.float64})\n                x = df.acoustic_data.values[-self.chunk_size:]\n                del df\n                yield seg_id, x, -999\n                \n    \n    def add_trend_feature(self,x,abs_values=False):\n        idx = np.array(range(len(x)))\n        if abs_values:\n            x = np.abs(x)\n        lr = LinearRegression()\n        lr.fit(idx.reshape(-1, 1), x)\n        return lr.coef_[0]\n    \n    def classic_sta_lta(self,x,length_sta, length_lta):\n        sta = np.cumsum(x ** 2)\n        # Convert to float\n        sta = np.require(sta, dtype=np.float)\n        # Copy for LTA\n        lta = sta.copy()\n        # Compute the STA and the LTA\n        sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n        sta /= length_sta\n        lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n        lta /= length_lta\n        # Pad zeros\n        sta[:length_lta - 1] = 0\n        # Avoid division by zero by setting zero values to tiny float\n        dtiny = np.finfo(0.0).tiny\n        idx = lta < dtiny\n        lta[idx] = dtiny\n        return sta / lta\n    \n    def calc_change_rate(self,x):\n        change = (np.diff(x) / x[:-1]).values\n        change = change[np.nonzero(change)[0]]\n        change = change[~np.isnan(change)]\n        change = change[change != -np.inf]\n        change = change[change != np.inf]\n        return np.mean(change)\n\n    def features(self, x, y, seg_id):\n        \n        feature_dict           = dict()\n        feature_dict['target'] = y\n        feature_dict['seg_id'] = seg_id\n        \n        #trend features\n\n        # create features here # numpy\n        feature_dict['mean'] = np.mean(x)\n        feature_dict['max'] = np.max(x)\n        feature_dict['min'] = np.min(x)\n        feature_dict['std'] = np.std(x)\n        feature_dict['var'] = np.var(x)\n        feature_dict['ptp'] = np.ptp(x)\n        feature_dict['percentile_10'] = np.percentile(x, 10)\n        feature_dict['percentile_20'] = np.percentile(x, 20)\n        feature_dict['percentile_30'] = np.percentile(x, 30)\n        feature_dict['percentile_40'] = np.percentile(x, 40)\n        feature_dict['percentile_50'] = np.percentile(x, 50)\n        feature_dict['percentile_60'] = np.percentile(x, 60)\n        feature_dict['percentile_70'] = np.percentile(x, 70)\n        feature_dict['percentile_80'] = np.percentile(x, 80)\n        feature_dict['percentile_90'] = np.percentile(x, 90)\n\n        # scipy\n        feature_dict['skew'] = sp.stats.skew(x)\n        feature_dict['kurtosis'] = sp.stats.kurtosis(x)\n        feature_dict['kstat_1'] = sp.stats.kstat(x, 1)\n        feature_dict['kstat_2'] = sp.stats.kstat(x, 2)\n        feature_dict['kstat_3'] = sp.stats.kstat(x, 3)\n        feature_dict['kstat_4'] = sp.stats.kstat(x, 4)\n        feature_dict['moment_1'] = sp.stats.moment(x, 1)\n        feature_dict['moment_2'] = sp.stats.moment(x, 2)\n        feature_dict['moment_3'] = sp.stats.moment(x, 3)\n        feature_dict['moment_4'] = sp.stats.moment(x, 4)\n        \n        feature_dict['abs_energy']         = feature_calculators.abs_energy(x)\n        feature_dict['abs_sum_of_changes'] = feature_calculators.absolute_sum_of_changes(x)\n        feature_dict['count_above_mean']    = feature_calculators.count_above_mean(x)\n        feature_dict['count_below_mean'] = feature_calculators.count_below_mean(x)\n        feature_dict['mean_abs_change'] = feature_calculators.mean_abs_change(x)\n        feature_dict['mean_change'] = feature_calculators.mean_change(x)\n        feature_dict['var_larger_than_std_dev'] = feature_calculators.variance_larger_than_standard_deviation(x)\n        feature_dict['range_minf_m4000'] = feature_calculators.range_count(x, -np.inf, -4000)\n        feature_dict['range_m4000_m3000'] = feature_calculators.range_count(x, -4000, -3000)\n        feature_dict['range_m3000_m2000'] = feature_calculators.range_count(x, -3000, -2000)\n        feature_dict['range_m2000_m1000'] = feature_calculators.range_count(x, -2000, -1000)\n        feature_dict['range_m1000_0'] = feature_calculators.range_count(x, -1000, 0)\n        feature_dict['range_0_p1000'] = feature_calculators.range_count(x, 0, 1000)\n        feature_dict['range_p1000_p2000'] = feature_calculators.range_count(x, 1000, 2000)\n        feature_dict['range_p2000_p3000'] = feature_calculators.range_count(x, 2000, 3000)\n        feature_dict['range_p3000_p4000'] = feature_calculators.range_count(x, 3000, 4000)\n        feature_dict['range_p4000_pinf'] = feature_calculators.range_count(x, 4000, np.inf)\n\n        feature_dict['ratio_unique_values'] = feature_calculators.ratio_value_number_to_time_series_length(x)\n        feature_dict['first_loc_min'] = feature_calculators.first_location_of_minimum(x)\n        feature_dict['first_loc_max'] = feature_calculators.first_location_of_maximum(x)\n        feature_dict['last_loc_min'] = feature_calculators.last_location_of_minimum(x)\n        feature_dict['last_loc_max'] = feature_calculators.last_location_of_maximum(x)\n        feature_dict['time_rev_asym_stat_10'] = feature_calculators.time_reversal_asymmetry_statistic(x, 10)\n        feature_dict['time_rev_asym_stat_100'] = feature_calculators.time_reversal_asymmetry_statistic(x, 100)\n        feature_dict['time_rev_asym_stat_1000'] = feature_calculators.time_reversal_asymmetry_statistic(x, 1000)\n        feature_dict['autocorrelation_5'] = feature_calculators.autocorrelation(x, 5)\n        feature_dict['autocorrelation_10'] = feature_calculators.autocorrelation(x, 10)\n        feature_dict['autocorrelation_50'] = feature_calculators.autocorrelation(x, 50)\n        feature_dict['autocorrelation_100'] = feature_calculators.autocorrelation(x, 100)\n        feature_dict['autocorrelation_1000'] = feature_calculators.autocorrelation(x, 1000)\n        feature_dict['c3_5'] = feature_calculators.c3(x, 5)\n        feature_dict['c3_10'] = feature_calculators.c3(x, 10)\n        feature_dict['c3_100'] = feature_calculators.c3(x, 100)\n        feature_dict['fft_1_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'real'}]))[0][1]\n        feature_dict['fft_1_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'imag'}]))[0][1]\n        feature_dict['fft_1_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'angle'}]))[0][1]\n        feature_dict['fft_2_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'real'}]))[0][1]\n        feature_dict['fft_2_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'imag'}]))[0][1]\n        feature_dict['fft_2_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'angle'}]))[0][1]\n        feature_dict['fft_3_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'real'}]))[0][1]\n        feature_dict['fft_3_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'imag'}]))[0][1]\n        feature_dict['fft_3_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'angle'}]))[0][1]\n        feature_dict['long_strk_above_mean'] = feature_calculators.longest_strike_above_mean(x)\n        feature_dict['long_strk_below_mean'] = feature_calculators.longest_strike_below_mean(x)\n        feature_dict['cid_ce_0'] = feature_calculators.cid_ce(x, 0)\n        feature_dict['cid_ce_1'] = feature_calculators.cid_ce(x, 1)\n        feature_dict['binned_entropy_5'] = feature_calculators.binned_entropy(x, 5)\n        feature_dict['binned_entropy_10'] = feature_calculators.binned_entropy(x, 10)\n        feature_dict['binned_entropy_20'] = feature_calculators.binned_entropy(x, 20)\n        feature_dict['binned_entropy_50'] = feature_calculators.binned_entropy(x, 50)\n        feature_dict['binned_entropy_80'] = feature_calculators.binned_entropy(x, 80)\n        feature_dict['binned_entropy_100'] = feature_calculators.binned_entropy(x, 100)\n\n        feature_dict['num_crossing_0'] = feature_calculators.number_crossing_m(x, 0)\n        feature_dict['num_peaks_10'] = feature_calculators.number_peaks(x, 10)\n        feature_dict['num_peaks_50'] = feature_calculators.number_peaks(x, 50)\n        feature_dict['num_peaks_100'] = feature_calculators.number_peaks(x, 100)\n        feature_dict['num_peaks_500'] = feature_calculators.number_peaks(x, 500)\n\n        feature_dict['spkt_welch_density_1'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 1}]))[0][1]\n        feature_dict['spkt_welch_density_10'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 10}]))[0][1]\n        feature_dict['spkt_welch_density_50'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 50}]))[0][1]\n        feature_dict['spkt_welch_density_100'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 100}]))[0][1]\n\n        feature_dict['time_rev_asym_stat_1'] = feature_calculators.time_reversal_asymmetry_statistic(x, 1)\n        feature_dict['time_rev_asym_stat_10'] = feature_calculators.time_reversal_asymmetry_statistic(x, 10)\n        feature_dict['time_rev_asym_stat_100'] = feature_calculators.time_reversal_asymmetry_statistic(x, 100)\n        \n        feature_dict['mean_change_rate'] = self.calc_change_rate(x)\n        feature_dict['abs_max']          = np.abs(x).max()\n        feature_dict['abs_min']          = np.abs(x).min()\n    \n        feature_dict['std_first_50000']  = x[:50000].std()\n        feature_dict['std_last_50000']   = x[-50000:].std()\n        feature_dict['std_first_10000']  = x[:10000].std()\n        feature_dict['std_last_10000']   = x[-10000:].std()\n    \n        feature_dict['avg_first_50000']  = x[:50000].mean()\n        feature_dict['avg_last_50000']   =  x[-50000:].mean()\n        feature_dict['avg_first_10000']  = x[:10000].mean()\n        feature_dict['avg_last_10000']   = x[-10000:].mean()\n    \n        feature_dict['min_first_50000'] = x[:50000].min()\n        feature_dict['min_last_50000']  = x[-50000:].min()\n        feature_dict['min_first_10000'] = x[:10000].min()\n        feature_dict['min_last_10000']  = x[-10000:].min()\n    \n        feature_dict['max_first_50000'] = x[:50000].max()\n        feature_dict['max_last_50000']  = x[-50000:].max()\n        feature_dict['max_first_10000'] = x[:10000].max()\n        feature_dict['max_last_10000']  = x[-10000:].max()\n    \n        feature_dict['max_to_min']      = x.max() / np.abs(x.min())\n        feature_dict['max_to_min_diff'] = x.max() - np.abs(x.min())\n        feature_dict['count_big']       = len(x[np.abs(x) > 500])\n        feature_dict['sum']             = x.sum()\n\n        feature_dict['mean_change_rate_first_50000'] = self.calc_change_rate(x[:50000])\n        feature_dict['mean_change_rate_last_50000']  = self.calc_change_rate(x[-50000:])\n        feature_dict['mean_change_rate_first_10000'] = self.calc_change_rate(x[:10000])\n        feature_dict['mean_change_rate_last_10000']  = self.calc_change_rate(x[-10000:])\n\n\n        feature_dict['abs_q95'] = np.quantile(np.abs(x), 0.95)\n        feature_dict['abs_q99'] = np.quantile(np.abs(x), 0.99)\n        feature_dict['abs_q05'] = np.quantile(np.abs(x), 0.05)\n        feature_dict['abs_q01'] = np.quantile(np.abs(x), 0.01)\n\n        feature_dict['trend']   = self.add_trend_feature(x)\n        feature_dict['abs_trend'] = self.add_trend_feature(x, abs_values=True)\n        feature_dict['abs_mean'] = np.abs(x).mean()\n        feature_dict['abs_std'] = np.abs(x).std()\n\n        feature_dict['mad'] = x.mad()\n        feature_dict['med'] = x.median()\n\n        feature_dict['Hilbert_mean'] = np.abs(hilbert(x)).mean()\n        feature_dict['Hann_window_mean'] = (convolve(x, hann(150), mode='same') / sum(hann(150))).mean()\n        feature_dict['classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n        feature_dict['classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n        feature_dict['classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n        feature_dict['classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n        feature_dict['classic_sta_lta5_mean'] = classic_sta_lta(x, 50, 1000).mean()\n        feature_dict['classic_sta_lta6_mean'] = classic_sta_lta(x, 100, 5000).mean()\n        feature_dict['classic_sta_lta7_mean'] = classic_sta_lta(x, 333, 666).mean()\n        feature_dict['classic_sta_lta8_mean'] = classic_sta_lta(x, 4000, 10000).mean()\n        feature_dict['Moving_average_700_mean'] = x.rolling(window=700).mean().mean(skipna=True)\n        feature_dict['exp_Moving_average_300_mean']    = (ewma(x, span=300).mean()).mean(skipna=True)\n        feature_dict['exp_Moving_average_3000_mean']   = ewma(x, span=3000).mean().mean(skipna=True)\n        feature_dict['exp_Moving_average_30000_mean']  = ewma(x, span=30000).mean().mean(skipna=True)\n        feature_dict['MA_700MA_std_mean']              = x.rolling(window=700).std().mean()\n        feature_dict['MA_700MA_BB_high_mean'] = feature_dict['Moving_average_700_mean'] + no_of_std*feature_dict['MA_700MA_std_mean']\n        feature_dict['MA_700MA_BB_low_mean'] = feature_dict['Moving_average_700_mean'] - no_of_std*feature_dict['MA_700MA_std_mean']\n        feature_dict['MA_400MA_std_mean']              = x.rolling(window=400).std().mean()\n        feature_dict['MA_400MA_BB_high_mean']= feature_dict['Moving_average_700_mean'] + no_of_std*feature_dict['MA_400MA_std_mean']\n        feature_dict['MA_400MA_BB_low_mean'] = feature_dict['Moving_average_700_mean'] - no_of_std*feature_dict['MA_400MA_std_mean']\n        feature_dict['MA_1000MA_std_mean']             = x.rolling(window=1000).std().mean()\n    \n\n        feature_dict['iqr']    = np.subtract(*np.percentile(x, [75, 25]))\n        feature_dict['q999']   = np.quantile(x,0.999)\n        feature_dict['q001']   = np.quantile(x,0.001)\n        feature_dict['ave10']  = stats.trim_mean(x, 0.1)\n\n        for windows in [10, 100, 1000]:\n            x_roll_std = x.rolling(windows).std().dropna().values\n            x_roll_mean = x.rolling(windows).mean().dropna().values\n\n            feature_dict['ave_roll_std_' + str(windows)] = x_roll_std.mean()\n            feature_dict['std_roll_std_' + str(windows)] = x_roll_std.std()\n            feature_dict['max_roll_std_' + str(windows)] = x_roll_std.max()\n            feature_dict['min_roll_std_' + str(windows)] = x_roll_std.min()\n            feature_dict['q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n            feature_dict['q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n            feature_dict['q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n            feature_dict['q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n            feature_dict['av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n            feature_dict['av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n            feature_dict['abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n\n            feature_dict['ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n            feature_dict['std_roll_mean_' + str(windows)] = x_roll_mean.std()\n            feature_dict['max_roll_mean_' + str(windows)] = x_roll_mean.max()\n            feature_dict['min_roll_mean_' + str(windows)] = x_roll_mean.min()\n            feature_dict['q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n            feature_dict['q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n            feature_dict['q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n            feature_dict['q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n            feature_dict['av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n            feature_dict['av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n            feature_dict['abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n\n        return feature_dict\n\n    def generate(self):\n        feature_list = []\n        res = Parallel(n_jobs=self.n_jobs,\n                       backend='threading')(delayed(self.features)(x, y, s)\n                                            for s, x, y in tqdm(self.read_chunks(), total=self.total_data))\n        for r in res:\n            feature_list.append(r)\n        return pd.DataFrame(feature_list)\n\n\ntraining_fg = FeatureGenerator(dtype='train', n_jobs=10, chunk_size=150000)\ntraining_data = training_fg.generate()\n\ntest_fg = FeatureGenerator(dtype='test', n_jobs=10, chunk_size=150000)\ntest_data = test_fg.generate()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}